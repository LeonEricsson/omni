{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1531, 0.2635, 0.4024, 0.4633, 0.3186, 0.4551, 0.4638, 0.0835],\n",
       "        [0.1392, 0.4005, 0.0702, 0.5426, 0.5568, 0.3502, 0.2161, 0.2036],\n",
       "        [0.4334, 0.3428, 0.4093, 0.5532, 0.0529, 0.1622, 0.3688, 0.2364],\n",
       "        [0.4908, 0.5195, 0.0566, 0.2100, 0.4331, 0.2564, 0.3644, 0.2362],\n",
       "        [0.3070, 0.0146, 0.5998, 0.0007, 0.3331, 0.3968, 0.0861, 0.5196]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.functional.normalize(torch.rand(5, 8), dim=1)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 8\n",
    "d_head = 4\n",
    "\n",
    "W = nn.Linear(d_model, d_head, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3099,  0.2017, -0.3154, -0.3220, -0.4352, -0.5511,  0.3520, -0.2075],\n",
       "        [-0.4646,  0.0176,  0.3620, -0.0090,  0.6122, -0.1070,  0.2112, -0.4711],\n",
       "        [-0.1046,  0.5039,  0.1310, -0.0678, -0.0075,  0.4254,  0.5461, -0.4839],\n",
       "        [-0.4674, -0.3934,  0.1226, -0.0053,  0.0143,  0.5657,  0.5386,  0.0382]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.normalize(W.weight.data, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5702, 0.5739, 0.5974, 0.6178])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7522, 0.7858, 0.8351, 0.9270, 0.8443, 0.7611, 0.7335, 0.6558])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.norm(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omni.preprocessing.tokenizer import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.create(\"EleutherAI/gpt-neo-125m\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(sequence_length: int, dtype=torch.float32):\n",
    "    mask = torch.triu(torch.ones((1, 1, sequence_length, sequence_length), dtype=dtype), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float(\"-inf\")) \n",
    "    return mask\n",
    "\n",
    "mask = causal_attention_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"data/pretokenized_fineweb-edu-2BT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x == 50257).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 15):\n",
    "\n",
    "    print(tokenizer.decode(ds[i][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from omni.architectures.llama import LlamaConfig\n",
    "from omni.modules.transformer import Transformer\n",
    "from omni.preprocessing.tokenizer import AutoTokenizer\n",
    "from omni.utils.system import auto_device\n",
    "\n",
    "tokenizer = AutoTokenizer.create(\"EleutherAI/gpt-neo-125m\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "\n",
    "llama_config = LlamaConfig(\n",
    "    vocab_size=50258,\n",
    "    seq_len=512,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    num_kv_heads=8,\n",
    "    num_layers=4,\n",
    "    rope_theta=0.1,\n",
    "    norm_eps=1e-6,\n",
    "    activation_fn=\"silu\",\n",
    "    mlp_bias=False,\n",
    "    mlp_dropout=0.0,\n",
    "    attention_bias=False,\n",
    "    attention_dropout=0.0,\n",
    "    pos_encoding_type=\"rope\",\n",
    "    mlp=\"mlp_swiglu\",\n",
    "    normalization=\"rmsnorm\",\n",
    "    attention=\"gqa\",\n",
    ")\n",
    "\n",
    "model = Transformer(llama_config)\n",
    "\n",
    "## create KV cache\n",
    "\n",
    "checkpoint = torch.load(\"checkpoints/llama-30M_20250123_104138/init.ckpt\", weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "device=\"mps\"\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"mps\"\n",
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids)\n",
    "next_token_logits = outputs[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_values, top_k_indices = torch.topk(logits, self.top_k, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_values = torch.tensor([float(\"-inf\"), 1.0, 2.0])\n",
    "\n",
    "top_k_values\n",
    "nn.functional.softmax(top_k_values, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_logits, sorted_indices = torch.sort(\n",
    "    top_k_values, dim=-1, descending=True\n",
    ")\n",
    "cumulative_probs = torch.cumsum(\n",
    "    nn.functional.softmax(sorted_logits, dim=-1), dim=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_logit_distribution(next_token_logits[0], top_k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_logit_distribution(logits, top_k=500):\n",
    "    \"\"\"\n",
    "    Visualizes the logit distribution by focusing on the top-k logits.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits tensor of shape (vocab_size,).\n",
    "        top_k (int): Number of top logits to visualize. Defaults to 500.\n",
    "    \"\"\"\n",
    "    if logits.dim() != 1:\n",
    "        raise ValueError(\"Logits tensor must be 1-dimensional (vocab_size,).\")\n",
    "\n",
    "    # Convert logits to probabilities and take log\n",
    "    log_probs = torch.log_softmax(logits, dim=0)\n",
    "\n",
    "    # Get the top-k log probabilities and their indices\n",
    "    top_log_probs, top_indices = torch.topk(log_probs, k=top_k)\n",
    "\n",
    "    # Sort the top-k log probabilities for visualization\n",
    "    sorted_log_probs, sorted_indices = torch.sort(top_log_probs, descending=True)\n",
    "\n",
    "    # Plot the distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sorted_log_probs.detach().cpu().numpy(), marker=\"o\", linestyle=\"-\")\n",
    "    plt.title(f\"Log Probability Distribution (Top {top_k})\", fontsize=14)\n",
    "    plt.xlabel(\"Rank\", fontsize=12)\n",
    "    plt.ylabel(\"Log Probability Value\", fontsize=12)\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers.processors as processors\n",
    "\n",
    "def _add_bos_token(tokenizer: AutoTokenizer):\n",
    "    tokenizer._tokenizer.post_processor = processors.Sequence(\n",
    "        [\n",
    "            processors.ByteLevel(trim_offsets=False),\n",
    "            processors.TemplateProcessing(\n",
    "                single=f\"{tokenizer.bos_token}:0 $A:0\",\n",
    "                pair=f\"{tokenizer.bos_token}:0 $A:0 {tokenizer.bos_token}:1 $B:1\",\n",
    "                special_tokens=[\n",
    "                    (tokenizer.bos_token, tokenizer.bos_token_id),\n",
    "                ],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = _add_bos_token(tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(torch.tensor(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"thjis is a test, this is a test\"\n",
    "\n",
    "output = tokenizer2(test, padding=\"max_length\", max_length=3, truncation=True, return_overflowing_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=128\n",
    "hidden_dim = 4 * int(2 * d_model / 3)\n",
    "hidden_dim = 4 * (\n",
    "    (hidden_dim + 4 - 1) // 4\n",
    ")\n",
    "\n",
    "hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "logits = torch.tensor([1,5,10], dtype=torch.float32)\n",
    "torch.softmax(logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(x, padding=\"max_length\", max_length=max_length, truncation=True, return_overflowing_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omni.modules.pos_embeddings import precompute_freqs_cis_real\n",
    "\n",
    "pos_embeddings = precompute_freqs_cis_real(64, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(dataset, tokenizer: AutoTokenizer, num_proc):\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tokenizer(\n",
    "            x[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            return_overflowing_tokens=True,\n",
    "        ),\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\", split=\"train\", num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenize(ds, tokenizer, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
