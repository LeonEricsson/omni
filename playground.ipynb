{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((512, 1024))\n",
    "\n",
    "x = a.split(2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_kv = torch.load(\"k.tensor\")\n",
    "kv = torch.load(\"k_kv.tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.abs(kv - non_kv).max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_kv_l5 = torch.load(\"k_layer5.tensor\")\n",
    "kv_l5 = torch.load(\"k_kv_layer5.tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff at layer 0: 1.5497207641601562e-06\n",
      "diff at layer 1: 0.5641153454780579\n",
      "diff at layer 2: 0.8714048862457275\n",
      "diff at layer 3: 0.8938214182853699\n",
      "diff at layer 4: 1.0913362503051758\n",
      "diff at layer 5: 1.0506775379180908\n"
     ]
    }
   ],
   "source": [
    "layers = 6\n",
    "\n",
    "for i in range(layers):\n",
    "    kv = torch.load(f\"out_kv_layer{i}.tensor\", weights_only=True)\n",
    "    non_kv = torch.load(f\"out_layer{i}.tensor\", weights_only=True)\n",
    "    print(f\"diff at layer {i}: {torch.abs(kv - non_kv).max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff at layer 0: 1.7285346984863281e-06\n",
      "diff at layer 1: 0.514729380607605\n",
      "diff at layer 2: 0.8702945709228516\n",
      "diff at layer 3: 0.8971630334854126\n",
      "diff at layer 4: 1.091021180152893\n",
      "diff at layer 5: 1.0824984312057495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/tk4xwnt92g588pqyvts_spwr0000gn/T/ipykernel_15908/1422871628.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  kv = torch.load(f\"k_kv_layer{i}.tensor\")\n",
      "/var/folders/fw/tk4xwnt92g588pqyvts_spwr0000gn/T/ipykernel_15908/1422871628.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  non_kv = torch.load(f\"k_layer{i}.tensor\")\n"
     ]
    }
   ],
   "source": [
    "layers = 6\n",
    "\n",
    "for i in range(layers):\n",
    "    kv = torch.load(f\"k_kv_layer{i}.tensor\")\n",
    "    non_kv = torch.load(f\"k_layer{i}.tensor\")\n",
    "    print(f\"diff at layer {i}: {torch.abs(kv - non_kv).max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(kv[0], non_kv[0], atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv[:, :-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv[0, 0, 5, 0], non_kv[0, 0, 5, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omni.preprocessing.tokenizer import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.create(\"EleutherAI/gpt-neo-125m\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from omni.architectures.llama import LlamaConfig\n",
    "from omni.modules.transformer import Transformer\n",
    "from omni.preprocessing.tokenizer import AutoTokenizer\n",
    "from omni.utils.system import auto_device\n",
    "\n",
    "tokenizer = AutoTokenizer.create(\"EleutherAI/gpt-neo-125m\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "\n",
    "llama_config = LlamaConfig(\n",
    "    vocab_size=50258,\n",
    "    seq_len=512,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    num_kv_heads=8,\n",
    "    num_layers=4,\n",
    "    rope_theta=0.1,\n",
    "    norm_eps=1e-6,\n",
    "    activation_fn=\"silu\",\n",
    "    mlp_bias=False,\n",
    "    mlp_dropout=0.0,\n",
    "    attention_bias=False,\n",
    "    attention_dropout=0.0,\n",
    "    pos_encoding_type=\"rope\",\n",
    "    mlp=\"mlp_swiglu\",\n",
    "    normalization=\"rmsnorm\",\n",
    "    attention=\"gqa\",\n",
    ")\n",
    "\n",
    "model = Transformer(llama_config)\n",
    "\n",
    "## create KV cache\n",
    "\n",
    "checkpoint = torch.load(\"checkpoints/llama-30M_20250123_104138/init.ckpt\", weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "device=\"mps\"\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"mps\"\n",
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids)\n",
    "next_token_logits = outputs[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_values, top_k_indices = torch.topk(logits, self.top_k, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_values = torch.tensor([float(\"-inf\"), 1.0, 2.0])\n",
    "\n",
    "top_k_values\n",
    "nn.functional.softmax(top_k_values, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_logits, sorted_indices = torch.sort(\n",
    "    top_k_values, dim=-1, descending=True\n",
    ")\n",
    "cumulative_probs = torch.cumsum(\n",
    "    nn.functional.softmax(sorted_logits, dim=-1), dim=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_logit_distribution(next_token_logits[0], top_k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_logit_distribution(logits, top_k=500):\n",
    "    \"\"\"\n",
    "    Visualizes the logit distribution by focusing on the top-k logits.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits tensor of shape (vocab_size,).\n",
    "        top_k (int): Number of top logits to visualize. Defaults to 500.\n",
    "    \"\"\"\n",
    "    if logits.dim() != 1:\n",
    "        raise ValueError(\"Logits tensor must be 1-dimensional (vocab_size,).\")\n",
    "\n",
    "    # Convert logits to probabilities and take log\n",
    "    log_probs = torch.log_softmax(logits, dim=0)\n",
    "\n",
    "    # Get the top-k log probabilities and their indices\n",
    "    top_log_probs, top_indices = torch.topk(log_probs, k=top_k)\n",
    "\n",
    "    # Sort the top-k log probabilities for visualization\n",
    "    sorted_log_probs, sorted_indices = torch.sort(top_log_probs, descending=True)\n",
    "\n",
    "    # Plot the distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sorted_log_probs.detach().cpu().numpy(), marker=\"o\", linestyle=\"-\")\n",
    "    plt.title(f\"Log Probability Distribution (Top {top_k})\", fontsize=14)\n",
    "    plt.xlabel(\"Rank\", fontsize=12)\n",
    "    plt.ylabel(\"Log Probability Value\", fontsize=12)\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers.processors as processors\n",
    "\n",
    "def _add_bos_token(tokenizer: AutoTokenizer):\n",
    "    tokenizer._tokenizer.post_processor = processors.Sequence(\n",
    "        [\n",
    "            processors.ByteLevel(trim_offsets=False),\n",
    "            processors.TemplateProcessing(\n",
    "                single=f\"{tokenizer.bos_token}:0 $A:0\",\n",
    "                pair=f\"{tokenizer.bos_token}:0 $A:0 {tokenizer.bos_token}:1 $B:1\",\n",
    "                special_tokens=[\n",
    "                    (tokenizer.bos_token, tokenizer.bos_token_id),\n",
    "                ],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = _add_bos_token(tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(torch.tensor(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"thjis is a test, this is a test\"\n",
    "\n",
    "output = tokenizer2(test, padding=\"max_length\", max_length=3, truncation=True, return_overflowing_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=128\n",
    "hidden_dim = 4 * int(2 * d_model / 3)\n",
    "hidden_dim = 4 * (\n",
    "    (hidden_dim + 4 - 1) // 4\n",
    ")\n",
    "\n",
    "hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "logits = torch.tensor([1,5,10], dtype=torch.float32)\n",
    "torch.softmax(logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(x, padding=\"max_length\", max_length=max_length, truncation=True, return_overflowing_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omni.modules.pos_embeddings import precompute_freqs_cis_real\n",
    "\n",
    "pos_embeddings = precompute_freqs_cis_real(64, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = torch.pow(2.0, torch.tensor(-8/16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(dataset, tokenizer: AutoTokenizer, num_proc):\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tokenizer(\n",
    "            x[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            return_overflowing_tokens=True,\n",
    "        ),\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\", split=\"train\", num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenize(ds, tokenizer, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
