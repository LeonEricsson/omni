{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'n_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mngpt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mn_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nTransformer, nConfig\n\u001b[1;32m      3\u001b[0m model_config \u001b[38;5;241m=\u001b[39m nConfig(\n\u001b[1;32m      4\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50258\u001b[39m,\n\u001b[1;32m      5\u001b[0m     seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     weight_tying\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m nTransformer(model_config)\n",
      "File \u001b[0;32m~/git/leon/omni/experiments/ngpt/n_transformer.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjaxtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bool, Float, Int\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mn_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m causal_attention_mask, nGQA, nMHA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mn_mlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nMLPSwiGLU\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'n_attention'"
     ]
    }
   ],
   "source": [
    "from experiments.ngpt.n_transformer import nTransformer, nConfig\n",
    "\n",
    "model_config = nConfig(\n",
    "    vocab_size=50258,\n",
    "    seq_len=512,\n",
    "    d_model=768,\n",
    "    num_layers=8,\n",
    "    num_heads=8,\n",
    "    num_kv_heads=8,\n",
    "    mlp_bias=False,\n",
    "    mlp_dropout=0.0,\n",
    "    attention_bias=False,\n",
    "    attention_dropout=0.0,\n",
    "    weight_tying=False,\n",
    ")\n",
    "\n",
    "model = nTransformer(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omni.preprocessing.tokenizer import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.create(\"EleutherAI/gpt-neo-125m\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(sequence_length: int, dtype=torch.float32):\n",
    "    mask = torch.triu(torch.ones((1, 1, sequence_length, sequence_length), dtype=dtype), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float(\"-inf\")) \n",
    "    return mask\n",
    "\n",
    "mask = causal_attention_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"data/pretokenized_fineweb-edu-2BT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x == 50257).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 15):\n",
    "\n",
    "    print(tokenizer.decode(ds[i][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from omni.architectures.llama import LlamaConfig\n",
    "from omni.modules.transformer import Transformer\n",
    "from omni.preprocessing.tokenizer import AutoTokenizer\n",
    "from omni.utils.system import auto_device\n",
    "\n",
    "tokenizer = AutoTokenizer.create(\"EleutherAI/gpt-neo-125m\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "\n",
    "llama_config = LlamaConfig(\n",
    "    vocab_size=50258,\n",
    "    seq_len=512,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    num_kv_heads=8,\n",
    "    num_layers=4,\n",
    "    rope_theta=0.1,\n",
    "    norm_eps=1e-6,\n",
    "    activation_fn=\"silu\",\n",
    "    mlp_bias=False,\n",
    "    mlp_dropout=0.0,\n",
    "    attention_bias=False,\n",
    "    attention_dropout=0.0,\n",
    "    pos_encoding_type=\"rope\",\n",
    "    mlp=\"mlp_swiglu\",\n",
    "    normalization=\"rmsnorm\",\n",
    "    attention=\"gqa\",\n",
    ")\n",
    "\n",
    "model = Transformer(llama_config)\n",
    "\n",
    "## create KV cache\n",
    "\n",
    "checkpoint = torch.load(\"checkpoints/llama-30M_20250123_104138/init.ckpt\", weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "device=\"mps\"\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"mps\"\n",
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids)\n",
    "next_token_logits = outputs[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_values, top_k_indices = torch.topk(logits, self.top_k, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_values = torch.tensor([float(\"-inf\"), 1.0, 2.0])\n",
    "\n",
    "top_k_values\n",
    "nn.functional.softmax(top_k_values, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_logits, sorted_indices = torch.sort(\n",
    "    top_k_values, dim=-1, descending=True\n",
    ")\n",
    "cumulative_probs = torch.cumsum(\n",
    "    nn.functional.softmax(sorted_logits, dim=-1), dim=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_logit_distribution(next_token_logits[0], top_k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_logit_distribution(logits, top_k=500):\n",
    "    \"\"\"\n",
    "    Visualizes the logit distribution by focusing on the top-k logits.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits tensor of shape (vocab_size,).\n",
    "        top_k (int): Number of top logits to visualize. Defaults to 500.\n",
    "    \"\"\"\n",
    "    if logits.dim() != 1:\n",
    "        raise ValueError(\"Logits tensor must be 1-dimensional (vocab_size,).\")\n",
    "\n",
    "    # Convert logits to probabilities and take log\n",
    "    log_probs = torch.log_softmax(logits, dim=0)\n",
    "\n",
    "    # Get the top-k log probabilities and their indices\n",
    "    top_log_probs, top_indices = torch.topk(log_probs, k=top_k)\n",
    "\n",
    "    # Sort the top-k log probabilities for visualization\n",
    "    sorted_log_probs, sorted_indices = torch.sort(top_log_probs, descending=True)\n",
    "\n",
    "    # Plot the distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sorted_log_probs.detach().cpu().numpy(), marker=\"o\", linestyle=\"-\")\n",
    "    plt.title(f\"Log Probability Distribution (Top {top_k})\", fontsize=14)\n",
    "    plt.xlabel(\"Rank\", fontsize=12)\n",
    "    plt.ylabel(\"Log Probability Value\", fontsize=12)\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers.processors as processors\n",
    "\n",
    "def _add_bos_token(tokenizer: AutoTokenizer):\n",
    "    tokenizer._tokenizer.post_processor = processors.Sequence(\n",
    "        [\n",
    "            processors.ByteLevel(trim_offsets=False),\n",
    "            processors.TemplateProcessing(\n",
    "                single=f\"{tokenizer.bos_token}:0 $A:0\",\n",
    "                pair=f\"{tokenizer.bos_token}:0 $A:0 {tokenizer.bos_token}:1 $B:1\",\n",
    "                special_tokens=[\n",
    "                    (tokenizer.bos_token, tokenizer.bos_token_id),\n",
    "                ],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = _add_bos_token(tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(torch.tensor(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"thjis is a test, this is a test\"\n",
    "\n",
    "output = tokenizer2(test, padding=\"max_length\", max_length=3, truncation=True, return_overflowing_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=128\n",
    "hidden_dim = 4 * int(2 * d_model / 3)\n",
    "hidden_dim = 4 * (\n",
    "    (hidden_dim + 4 - 1) // 4\n",
    ")\n",
    "\n",
    "hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "logits = torch.tensor([1,5,10], dtype=torch.float32)\n",
    "torch.softmax(logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(x, padding=\"max_length\", max_length=max_length, truncation=True, return_overflowing_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omni.modules.pos_embeddings import precompute_freqs_cis_real\n",
    "\n",
    "pos_embeddings = precompute_freqs_cis_real(64, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(dataset, tokenizer: AutoTokenizer, num_proc):\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tokenizer(\n",
    "            x[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            return_overflowing_tokens=True,\n",
    "        ),\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\", split=\"train\", num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenize(ds, tokenizer, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
