{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, defaultdict, Counter, namedtuple\n",
    "\n",
    "word_counts = Counter(\"asdoijads\")\n",
    "\n",
    "c = Counter({'a': 2, 'b': 3})\n",
    "\n",
    "ring = deque([3,1,3], maxlen=3)\n",
    "\n",
    "ring.append(5)\n",
    "\n",
    "hist = defaultdict(int)\n",
    "\n",
    "Point = namedtuple(\"Point\", ['x', 'y'])\n",
    "\n",
    "Point(1,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import functools as ft\n",
    "\n",
    "\n",
    "count = it.count(0, 5)\n",
    "\n",
    "x = list(it.islice(count, 5))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked(iterable, size):\n",
    "    it = iter(iterable)\n",
    "    while chunk := list(it.islice(it, size)):\n",
    "        yield chunk\n",
    "\n",
    "with open(\"demo.jsonl\") as fh:\n",
    "    chunked(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import pathlib\n",
    "\n",
    "def chunked(iterable, size):\n",
    "    \"\"\"Yield successive lists (chunks) of length <= size.\"\"\"\n",
    "    it = iter(iterable)  # always use the real built-in\n",
    "    while (chunk := list(islice(it, size))):\n",
    "        yield chunk\n",
    "\n",
    "path = pathlib.Path('demo.jsonl')\n",
    "\n",
    "with path.open() as fh:\n",
    "    for batch in chunked(fh, 5):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = it.count(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = list(it.islice(c, 5))\n",
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "orig = [[1, 2], [3, 4]]\n",
    "shallow = copy.copy(orig)\n",
    "\n",
    "shallow[0] is orig[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "example = [\n",
    "    {\"visit_id\": \"v1\", \"doctor_id\": \"dr_42\",\n",
    "     \"note\": \"Patient stable. Continue meds.\"},\n",
    "    {\"visit_id\": \"v2\", \"doctor_id\": \"dr_99\",\n",
    "     \"note\": \"↑ BP, schedule follow-up in 2 weeks.\"},\n",
    "    {\"visit_id\": \"v3\", \"doctor_id\": \"dr_42\",\n",
    "     \"note\": \"No complications; discharge tomorrow.\"},\n",
    "]\n",
    "\n",
    "tokens = len(example[0][\"note\"].split())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VisitStats(doctor_id='dr_42', num_visits=2, total_tokens=8, avg_tokens_per_visit=4.0),\n",
       " VisitStats(doctor_id='dr_99', num_visits=1, total_tokens=7, avg_tokens_per_visit=7.0)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class VisitStats:\n",
    "    doctor_id: str\n",
    "    num_visits: int\n",
    "    total_tokens: int      \n",
    "    avg_tokens_per_visit: float\n",
    "\n",
    "def summarise_visits(visits: list[dict[str, str]]) -> list[VisitStats]:\n",
    "    docs = {}\n",
    "    for visit in visits:\n",
    "        tokens = len(visit[\"note\"].split())\n",
    "        if visit[\"doctor_id\"] in docs:\n",
    "            docs[visit[\"doctor_id\"]].num_visits += 1\n",
    "            docs[visit[\"doctor_id\"]].total_tokens += tokens\n",
    "        else:\n",
    "            docs[visit[\"doctor_id\"]] = VisitStats(\n",
    "                doctor_id=visit[\"doctor_id\"],\n",
    "                num_visits = 1,\n",
    "                total_tokens=tokens,\n",
    "                avg_tokens_per_visit=0\n",
    "            )\n",
    "            \n",
    "\n",
    "    visitstats = list(docs.values())\n",
    "    for vs in visitstats:\n",
    "        vs.avg_tokens_per_visit = vs.total_tokens / vs.num_visits\n",
    "    \n",
    "    return sorted(\n",
    "        visitstats, key=lambda a: (-a.total_tokens, a.doctor_id)\n",
    "    )\n",
    "\n",
    "\n",
    "example = [\n",
    "    {\"visit_id\": \"v1\", \"doctor_id\": \"dr_42\",\n",
    "     \"note\": \"Patient stable. Continue meds.\"},\n",
    "    {\"visit_id\": \"v2\", \"doctor_id\": \"dr_99\",\n",
    "     \"note\": \"↑ BP, schedule follow-up in 2 weeks.\"},\n",
    "    {\"visit_id\": \"v3\", \"doctor_id\": \"dr_42\",\n",
    "     \"note\": \"No complications; discharge tomorrow.\"},\n",
    "]\n",
    "\n",
    "summarise_visits(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heightChecker(heights) -> int:\n",
    "        # expected: List[int] of heights ascending order where expected[i] is the\n",
    "        # height of student i.\n",
    "        expected = sorted(heights) # o(n logn)\n",
    "        diff = 0\n",
    "        for i in range(len(expected)):\n",
    "            if expected[i] != heights[i]:\n",
    "                  diff += 1\n",
    "        return diff\n",
    "\n",
    "heights = [1,2,3,4,5]\n",
    "heightChecker(heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.ngpt.n_transformer import nTransformer, nConfig\n",
    "\n",
    "model_config = nConfig(\n",
    "    vocab_size=50258,\n",
    "    seq_len=512,\n",
    "    d_model=768,\n",
    "    num_layers=8,\n",
    "    num_heads=8,\n",
    "    num_kv_heads=8,\n",
    "    mlp_bias=False,\n",
    "    mlp_dropout=0.0,\n",
    "    attention_bias=False,\n",
    "    attention_dropout=0.0,\n",
    "    weight_tying=False,\n",
    ")\n",
    "\n",
    "model = nTransformer(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omni.preprocessing.tokenizer import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.create(\"EleutherAI/gpt-neo-125m\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(sequence_length: int, dtype=torch.float32):\n",
    "    mask = torch.triu(torch.ones((1, 1, sequence_length, sequence_length), dtype=dtype), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float(\"-inf\")) \n",
    "    return mask\n",
    "\n",
    "mask = causal_attention_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"data/pretokenized_fineweb-edu-2BT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x == 50257).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 15):\n",
    "\n",
    "    print(tokenizer.decode(ds[i][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from omni.architectures.llama import LlamaConfig\n",
    "from omni.modules.transformer import Transformer\n",
    "from omni.preprocessing.tokenizer import AutoTokenizer\n",
    "from omni.utils.system import auto_device\n",
    "\n",
    "tokenizer = AutoTokenizer.create(\"EleutherAI/gpt-neo-125m\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "\n",
    "llama_config = LlamaConfig(\n",
    "    vocab_size=50258,\n",
    "    seq_len=512,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    num_kv_heads=8,\n",
    "    num_layers=4,\n",
    "    rope_theta=0.1,\n",
    "    norm_eps=1e-6,\n",
    "    activation_fn=\"silu\",\n",
    "    mlp_bias=False,\n",
    "    mlp_dropout=0.0,\n",
    "    attention_bias=False,\n",
    "    attention_dropout=0.0,\n",
    "    pos_encoding_type=\"rope\",\n",
    "    mlp=\"mlp_swiglu\",\n",
    "    normalization=\"rmsnorm\",\n",
    "    attention=\"gqa\",\n",
    ")\n",
    "\n",
    "model = Transformer(llama_config)\n",
    "\n",
    "## create KV cache\n",
    "\n",
    "checkpoint = torch.load(\"checkpoints/llama-30M_20250123_104138/init.ckpt\", weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "device=\"mps\"\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"mps\"\n",
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids)\n",
    "next_token_logits = outputs[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_values, top_k_indices = torch.topk(logits, self.top_k, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_values = torch.tensor([float(\"-inf\"), 1.0, 2.0])\n",
    "\n",
    "top_k_values\n",
    "nn.functional.softmax(top_k_values, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_logits, sorted_indices = torch.sort(\n",
    "    top_k_values, dim=-1, descending=True\n",
    ")\n",
    "cumulative_probs = torch.cumsum(\n",
    "    nn.functional.softmax(sorted_logits, dim=-1), dim=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_logit_distribution(next_token_logits[0], top_k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_logit_distribution(logits, top_k=500):\n",
    "    \"\"\"\n",
    "    Visualizes the logit distribution by focusing on the top-k logits.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits tensor of shape (vocab_size,).\n",
    "        top_k (int): Number of top logits to visualize. Defaults to 500.\n",
    "    \"\"\"\n",
    "    if logits.dim() != 1:\n",
    "        raise ValueError(\"Logits tensor must be 1-dimensional (vocab_size,).\")\n",
    "\n",
    "    # Convert logits to probabilities and take log\n",
    "    log_probs = torch.log_softmax(logits, dim=0)\n",
    "\n",
    "    # Get the top-k log probabilities and their indices\n",
    "    top_log_probs, top_indices = torch.topk(log_probs, k=top_k)\n",
    "\n",
    "    # Sort the top-k log probabilities for visualization\n",
    "    sorted_log_probs, sorted_indices = torch.sort(top_log_probs, descending=True)\n",
    "\n",
    "    # Plot the distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sorted_log_probs.detach().cpu().numpy(), marker=\"o\", linestyle=\"-\")\n",
    "    plt.title(f\"Log Probability Distribution (Top {top_k})\", fontsize=14)\n",
    "    plt.xlabel(\"Rank\", fontsize=12)\n",
    "    plt.ylabel(\"Log Probability Value\", fontsize=12)\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers.processors as processors\n",
    "\n",
    "def _add_bos_token(tokenizer: AutoTokenizer):\n",
    "    tokenizer._tokenizer.post_processor = processors.Sequence(\n",
    "        [\n",
    "            processors.ByteLevel(trim_offsets=False),\n",
    "            processors.TemplateProcessing(\n",
    "                single=f\"{tokenizer.bos_token}:0 $A:0\",\n",
    "                pair=f\"{tokenizer.bos_token}:0 $A:0 {tokenizer.bos_token}:1 $B:1\",\n",
    "                special_tokens=[\n",
    "                    (tokenizer.bos_token, tokenizer.bos_token_id),\n",
    "                ],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = _add_bos_token(tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(torch.tensor(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"thjis is a test, this is a test\"\n",
    "\n",
    "output = tokenizer2(test, padding=\"max_length\", max_length=3, truncation=True, return_overflowing_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=128\n",
    "hidden_dim = 4 * int(2 * d_model / 3)\n",
    "hidden_dim = 4 * (\n",
    "    (hidden_dim + 4 - 1) // 4\n",
    ")\n",
    "\n",
    "hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "logits = torch.tensor([1,5,10], dtype=torch.float32)\n",
    "torch.softmax(logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(x, padding=\"max_length\", max_length=max_length, truncation=True, return_overflowing_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omni.modules.pos_embeddings import precompute_freqs_cis_real\n",
    "\n",
    "pos_embeddings = precompute_freqs_cis_real(64, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(dataset, tokenizer: AutoTokenizer, num_proc):\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tokenizer(\n",
    "            x[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            return_overflowing_tokens=True,\n",
    "        ),\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A bare-bones implementation of a Multi-Head Attention module\n",
    "    as described in Section 2.1 of arXiv:2505.06708v1.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, config = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): The total dimensionality of the model.\n",
    "            num_heads (int): The number of parallel attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Stage 1: QKV Linear Projections \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # Gate\n",
    "        self.W_q_gate = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k_gate = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v_gate = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # headwise score\n",
    "        self.W_q_headwise_gate = nn.Linear(d_model, num_heads, bias=False)\n",
    "        self.W_k_headwise_gate = nn.Linear(d_model, num_heads, bias=False)\n",
    "        self.W_v_headwise_gate = nn.Linear(d_model, num_heads, bias=False)\n",
    "        \n",
    "        # head shared \n",
    "        self.W_q_head_shared_gate = nn.Linear(d_model, self.head_dim, bias=False)\n",
    "        self.W_k_head_shared_gate = nn.Linear(d_model, self.head_dim, bias=False)\n",
    "        self.W_v_head_shared_gate = nn.Linear(d_model, self.head_dim, bias=False)\n",
    "\n",
    "        # Stage 4: Final Output Layer \n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # --- Stage 1: QKV Linear Projections ---\n",
    "        # Project input `x` into Q, K, V tensors.\n",
    "        # Shape: (batch_size, seq_len, d_model)\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        # gating QKV projections\n",
    "        q = q * torch.sigmoid(self.W_q_gate(x)) \n",
    "        k = k * torch.sigmoid(self.W_v_gate(x))  \n",
    "        v = v * torch.sigmoid(self.W_v_gate(x)) \n",
    "\n",
    "        # --- Stage 3: Multi-Head Splitting ---\n",
    "        # Reshape Q, K, V for multi-head processing.\n",
    "        # New Shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # headwise score\n",
    "        q = q * torch.sigmoid(self.W_q_headwise_gate(x)).view(batch_size, self.num_heads, seq_len, 1)\n",
    "        k = k * torch.sigmoid(self.W_k_headwise_gate(x)).view(batch_size, self.num_heads, seq_len, 1)\n",
    "        v = v * torch.sigmoid(self.W_k_headwise_gate(x)).view(batch_size, self.num_heads, seq_len, 1)\n",
    "\n",
    "        # head shared QKV gating\n",
    "        q = q * torch.sigmoid(self.W_q_head_shared_gate(x)).unsqueeze(1)\n",
    "        k = k * torch.sigmoid(self.W_k_head_shared_gate(x)).unsqueeze(1)\n",
    "        v = v * torch.sigmoid(self.W_k_head_shared_gate(x)).unsqueeze(1)\n",
    "\n",
    "        # --- Stage 2: Scaled Dot-Product Attention (SDPA) ---\n",
    "        # Shape of attn_scores: (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply softmax to get attention weights.\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply weights by V to get the attention output.\n",
    "        # Shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "        attention_output = attn_weights @ v\n",
    "\n",
    "        # --- Stage 3: Multi-Head Concatenation ---\n",
    "        # Reshape the attention output to concatenate heads.\n",
    "        # transpose(1, 2) -> (batch_size, seq_len, num_heads, head_dim)\n",
    "        # contiguous().view() -> (batch_size, seq_len, d_model)\n",
    "        concatenated_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # --- Stage 4: Final Output Projection ---\n",
    "        # Pass the concatenated output through the final linear layer.\n",
    "        # Shape: (batch_size, seq_len, d_model)\n",
    "        final_output = self.W_o(concatenated_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "        self,\n",
    "        x: Float[Tensor, \"batch seq d_model\"],\n",
    "        mask: Float[Tensor, \"1 1 seq seq\"],\n",
    "        pos_info: Optional[Tensor],\n",
    "        kv_cache,\n",
    "    ):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "\n",
    "        qkv = self.W_QKV(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        q = q.reshape(batch_size, seq_length, self.n_heads, self.head_dim)\n",
    "        k = k.reshape(batch_size, seq_length, self.n_heads, self.head_dim)\n",
    "        v = v.reshape(batch_size, seq_length, self.n_heads, self.head_dim)\n",
    "\n",
    "        # Apply gating mechanisms using direct config access.\n",
    "        if self.config.qkv_gate_enabled:\n",
    "            q = q * torch.sigmoid(self.W_q_gate(x)) \n",
    "            k = k * torch.sigmoid(self.W_v_gate(x))  \n",
    "            v = v * torch.sigmoid(self.W_v_gate(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1     # Number of sequences to process in parallel\n",
    "seq_len = 64     # The length of each input sequence\n",
    "d_model = 512    # The dimensionality of the model's embeddings\n",
    "num_heads = 4      # Number of attention heads (512 is divisible by 8)\n",
    "\n",
    "mha_baseline = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "dummy_x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output = mha_baseline(dummy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Complex, Float, Int\n",
    "from torch import Tensor\n",
    "\n",
    "from omni.modules.pos_embeddings import apply_rope_real\n",
    "\n",
    "AttentionType = Literal[\"mha\", \"gqa\"]\n",
    "\n",
    "def forward(\n",
    "        self,\n",
    "        x: Float[Tensor, \"batch seq d_model\"],\n",
    "        mask: Float[Tensor, \"1 1 seq seq\"],\n",
    "        pos_info: Optional[Tensor],\n",
    "        kv_cache,\n",
    "    ):\n",
    "        # ...\n",
    "    \n",
    "        output: Float[Tensor, \"batch num_heads seq head_dim\"] = F.scaled_dot_product_attention(\n",
    "            q, \n",
    "            k, \n",
    "            v, \n",
    "            attn_mask=mask, \n",
    "            dropout_p=self.attn_dropout.p if self.training else 0.0\n",
    "        )\n",
    "        \n",
    "        output = output.transpose(1, 2)\n",
    "        if self.config.attention_output_gate_enabled:\n",
    "            # self.W_attn_out_gate = nn.Linear(d_model, d_model)\n",
    "            gate_scores = torch.sigmoid(self.W_attn_out_gate(x))\n",
    "            output = output * gate_scores.reshape(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # ...\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
