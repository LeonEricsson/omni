# omni
my collection of paper implementations and experiments. built to be modular, easy to extend, and experiment with.


### papers

- [Multi-head Latent Attention](/experiments/mla/mla.py)
- [Differential Transformer](/experiments/differential_transformer/diff_attention.py)
- [Rotary Embeddings](/omni/modules/pos_embeddings.py#L59)
- [Attention with Linear Biases](/omni/modules/pos_embeddings.py#L176)
- [Llama](/omni/architectures/llama.py)
- [GPT](/omni/architectures/gpt.py)


### Llama 30M

<img src="/experiments/basic/assets/train_loss.png" alt="Train Loss" width="300">

### todo

- [ ] Differential Transformer
- [ ] MLA

### todo sides
- [ ] Swap config structure to importing modules instead
